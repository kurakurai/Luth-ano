base_model: "LiquidAI/LFM2-1.2B"

output_dir: "trained_model/"
deepspeed: configs/train/zero1.json # Multi-GPU training config

flash_attention: true
train_on_inputs: false
train_on_eos: 'turn'
sample_packing: true
chunked_cross_entropy: true
learning_rate: 5e-5
sequence_len: 16384  # larger sequence length improves packing efficiency for more tokens/sec
gradient_checkpointing: true # tradeoff reduced VRAM for increased time
optimizer: "adamw_torch_fused"
lr_scheduler: "cosine"
warmup_ratio: 0.1
weight_decay: 0.0
bf16: true
fp16: false
tf32: false
max_grad_norm: 0.1
num_epochs: 3
save_strategy: "epoch"
logging_steps: 1

val_set_size: 0.05
evals_per_epoch: 2

# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
gradient_accumulation_steps: 2
micro_batch_size: 10
# eval_batch_size: 4

use_wandb: true
wandb_project: ""
wandb_name: luth-lfm2-1.2B

chat_template: "tokenizer_default"
datasets:
  - path: ""
    type: "chat_template"
    split: "luth_scholar"
  - path: ""
    type: "chat_template"
    split: "luth_smoltalk2"
  - path: ""
    type: "chat_template"
    split: "luth_croissantllm"
  - path: "/luth-sft"
    type: "chat_template"
    split: "luth_aya_dataset"
  - path: "/luth-sft"
    type: "chat_template"
    split: "luth_tulu3_persona_math"
  - path: "/luth-sft"
    type: "chat_template"
    split: "luth_openhermes"
  - path: "/luth-sft"
    type: "chat_template"
    split: "luth_tulu3_persona_instruct"


eot_tokens:
  - "<|im_end|>"
dataloader_prefetch_factor: 8
dataloader_num_workers: 2
dataloader_pin_memory: true

special_tokens: