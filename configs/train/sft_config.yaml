base_model: "kurakurai/Qwen3-0.6B-Instruct"

output_dir: "trained_model/luth"
hf_use_auth_token: true
hub_model_id: "kurakurai/Luth-scholar-fr"
hub_strategy: "checkpoint"

deepspeed: configs/train/zero1.json # Multi-GPU training config

flash_attention: true
train_on_inputs: false
sample_packing: true
chunked_cross_entropy: true
learning_rate: 0.00001
sequence_len: 16384  # larger sequence length improves packing efficiency for more tokens/sec
gradient_checkpointing: true # tradeoff reduced VRAM for increased time
optimizer: "adamw_torch_8bit"
lr_scheduler: "cosine"
warmup_ratio: 0.2
bf16: true
fp16: false
tf32: false
max_grad_norm: 0.1
num_epochs: 3
saves_per_epoch: 1
logging_steps: 1

# lora_r: 16
# lora_alpha: 32
# lora_target_modules:
#   - "q_proj"
#   - "k_proj"
#   - "v_proj"
#   - "o_proj"

# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
gradient_accumulation_steps: 4
micro_batch_size: 6
# eval_batch_size: 4

use_wandb: true
wandb_project: "Kurakura AI"
wandb_name: luth-run-qwen3-0.6b

chat_template: "tokenizer_default"
datasets:
  - path: "kurakurai/Scholar-fr"
    type: "chat_template"
    split: "Scholar_all"
    field_messages: conversations

eot_tokens:
  - "<|im_end|>"
dataloader_prefetch_factor: 8
dataloader_num_workers: 2
dataloader_pin_memory: true