from lighteval.metrics.metrics import Metrics
from lighteval.tasks.extended.ifeval.main import ifeval_metrics
from lighteval.tasks.lighteval_task import LightevalTaskConfig
from lighteval.tasks.templates.multichoice import get_mcq_prompt_function
from lighteval.tasks.default_prompts import LETTER_INDICES
from lighteval.tasks.multilingual.utils.task_utils import get_metrics_for_formulation
from lighteval.metrics.dynamic_metrics import loglikelihood_acc_metric
from lighteval.metrics.normalizations import (
    LogProbCharNorm,
    LogProbPMINorm,
    LogProbTokenNorm,
)
import lighteval.tasks.default_prompts as default_prompts
from lighteval.utils.language import Language
from lighteval.tasks.templates.utils.formulation import MCFFormulation
import prompts as custom_prompt
import metrics as custom_metric
from metrics import MetricsThinking
import os

enable_thinking = os.environ.get("enable_thinking", "false").lower() == "true"
metrics_module = Metrics if not enable_thinking else MetricsThinking
metrics = [
    metrics_module.exact_match,
    metrics_module.quasi_exact_match,
    metrics_module.prefix_exact_match,
    metrics_module.prefix_quasi_exact_match,
]

# Arc-Challenge-fr task
arc_challenge_fr_task = LightevalTaskConfig(
    name="arc_challenge_fr",
    suite=["community"],
    prompt_function=custom_prompt.prompt_arc_fr,
    hf_repo="manu/french_bench_arc_challenge",
    hf_subset="default",
    hf_avail_splits=["train", "test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1 if not enable_thinking else 4096,
    metric=[Metrics.loglikelihood_acc, Metrics.loglikelihood_acc_norm_nospace],
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)

# AIME24-fr task
aime24_fr_task = LightevalTaskConfig(
    name="aime24_fr",
    suite=["community"],
    prompt_function=custom_prompt.prompt_aime_fr,
    hf_repo="/aime_2024_fr",
    hf_subset="default",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        custom_metric.math_fr_pass_at_1_1n,
    ],
    version=2,
)

# Math-500-fr task
math_500_fr_task = LightevalTaskConfig(
    name="math_500_fr",
    prompt_function=custom_prompt.prompt_math_500_fr,
    suite=["community"],
    hf_repo="bezir/MATH-500-multilingual",
    hf_subset="French",
    metric=[custom_metric.math_fr_pass_at_1_1n],
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    stop_sequence=[],  # no stop sequence, will use eot token
    version="0.1",
)


# kholle-fr task
kholle_fr_task = LightevalTaskConfig(
    name="kholle_fr",
    prompt_function=custom_prompt.prompt_kholle_fr,
    suite=["community"],
    hf_repo="/kholle",
    hf_subset="default",
    metric=[custom_metric.kholle_pass_at_1_1n],
    hf_avail_splits=["combined", "cpge", "bac"],
    evaluation_splits=["combined"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    stop_sequence=[],  # no stop sequence, will use eot token
    version="0.1",
)

# IFEVal-fr task
ifeval_fr_task = LightevalTaskConfig(
    name="ifeval_fr",
    prompt_function=custom_prompt.prompt_ifeval_fr,
    suite=["community"],
    hf_repo="jzhang86/fr_ifeval",
    hf_subset="default",
    metric=[ifeval_metrics],
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    stop_sequence=[],  # no stop sequence, will use eot token
    version="0.1",
)

# GPQA-Diamond-fr task
gpqa_diamond_fr_task = LightevalTaskConfig(
    name="gpqa_fr:diamond",
    suite=["community"],
    prompt_function=custom_prompt.gpqa_fr_instruct,
    hf_repo="le-leadboard/gpqa-fr",
    hf_subset="gpqa_diamond",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        custom_metric.gpqa_instruct_pass_fr_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)

gpqa_extended_fr_task = LightevalTaskConfig(
    name="gpqa_fr:extended",
    suite=["community"],
    prompt_function=custom_prompt.gpqa_fr_instruct,
    hf_repo="le-leadboard/gpqa-fr",
    hf_subset="gpqa_extended",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        custom_metric.gpqa_instruct_pass_fr_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)
gpqa_main_fr_task = LightevalTaskConfig(
    name="gpqa_fr:main",
    suite=["community"],
    prompt_function=custom_prompt.gpqa_fr_instruct,
    hf_repo="le-leadboard/gpqa-fr",
    hf_subset="gpqa_main",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        custom_metric.gpqa_instruct_pass_fr_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)

# Hellaswag-fr task
hellaswag_fr_task = LightevalTaskConfig(
    name="hellaswag_fr",
    suite=["community"],
    prompt_function=custom_prompt.prompt_hellaswag_fr,
    hf_repo="manu/french_bench_hellaswag",
    hf_subset="default",
    hf_avail_splits=["validation"],
    evaluation_splits=["validation"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)


# Math-Fr task
math_lvl5_fr_task = LightevalTaskConfig(
    name="math_lvl5_fr",
    suite=["community"],
    prompt_function=custom_prompt.prompt_math_fr,
    hf_repo="le-leadboard/MATH_LVL5_fr",
    hf_subset="default",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        custom_metric.math_fr_pass_at_1_1n,
    ],
    trust_dataset=True,
    version=0,
)

# BoolQ-fr task
boolq_fr_task = LightevalTaskConfig(
    name="boolq_fr",
    suite=["community"],
    prompt_function=custom_prompt.prompt_boolq_fr,
    hf_repo="manu/french_boolq",
    hf_subset="default",
    hf_avail_splits=["train", "valid"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=5 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)

# MuSR tasks
musr_fr_murder_mysteries = LightevalTaskConfig(
    name="musr_fr:murder_mysteries",
    suite=["community"],
    prompt_function=custom_prompt.musr_fr,
    hf_repo="le-leadboard/musr-fr",
    hf_subset="default",
    hf_avail_splits=["murder_mysteries"],
    evaluation_splits=["murder_mysteries"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1 if not enable_thinking else 4096,
    metric=[Metrics.loglikelihood_acc],
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)
musr_fr_object_placements = LightevalTaskConfig(
    name="musr_fr:object_placements",
    suite=["community"],
    prompt_function=custom_prompt.musr_fr,
    hf_repo="le-leadboard/musr-fr",
    hf_subset="default",
    hf_avail_splits=["object_placements"],
    evaluation_splits=["object_placements"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1 if not enable_thinking else 4096,
    metric=[Metrics.loglikelihood_acc],
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)
musr_fr_team_allocation = LightevalTaskConfig(
    name="musr_fr:team_allocation",
    suite=["community"],
    prompt_function=custom_prompt.musr_fr,
    hf_repo="le-leadboard/musr-fr",
    hf_subset="default",
    hf_avail_splits=["team_allocation"],
    evaluation_splits=["team_allocation"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=1 if not enable_thinking else 4096,
    metric=[Metrics.loglikelihood_acc],
    stop_sequence=[],
    trust_dataset=True,
    version=0,
)

# MMLU-fr task
mmlu_fr_task = LightevalTaskConfig(
    name="mmlu_fr",
    prompt_function=get_mcq_prompt_function(
        Language.FRENCH,
        lambda line: {
            "question": line["Question"],
            "choices": [line["A"], line["B"], line["C"], line["D"]],
            "gold_idx": LETTER_INDICES.index(line["Answer"]),
        },
        formulation=MCFFormulation(),
    ),
    suite=["community"],
    hf_repo="openai/MMMLU",
    hf_subset="FR_FR",
    evaluation_splits=["test"],
    hf_avail_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    hf_revision="038c7808122969ead7456361af05cb8f47d247f8",
    metric=get_metrics_for_formulation(
        MCFFormulation(),
        [
            loglikelihood_acc_metric(normalization=LogProbTokenNorm()),
            loglikelihood_acc_metric(normalization=LogProbCharNorm()),
            loglikelihood_acc_metric(normalization=LogProbPMINorm()),
        ],
    ),
)


# bbh_fr task
bbh_boolean_expressions_community = LightevalTaskConfig(
    name="bbh_fr:expressions_booléennes",
    suite=["community"],
    prompt_function=custom_prompt.bbh_boolean_expressions,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="expressions_booléennes",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_causal_judgment_community = LightevalTaskConfig(
    name="bbh_fr:jugement_causal",
    suite=["community"],
    prompt_function=custom_prompt.bbh_causal_judgment,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="jugement_causal",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_date_understanding_community = LightevalTaskConfig(
    name="bbh_fr:compréhension_de_la_date",
    suite=["community"],
    prompt_function=custom_prompt.bbh_date_understanding,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="compréhension_de_la_date",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_disambiguation_qa_community = LightevalTaskConfig(
    name="bbh_fr:désambiguïsation_qa",
    suite=["community"],
    prompt_function=custom_prompt.bbh_disambiguation_qa,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="désambiguïsation_qa",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_dyck_languages_community = LightevalTaskConfig(
    name="bbh_fr:dyck_languages",
    suite=["community"],
    prompt_function=custom_prompt.bbh_dyck_languages,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="dyck_languages",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_formal_fallacies_community = LightevalTaskConfig(
    name="bbh_fr:sophismes_formels",
    suite=["community"],
    prompt_function=custom_prompt.bbh_formal_fallacies,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="sophismes_formels",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_geometric_shapes_community = LightevalTaskConfig(
    name="bbh_fr:formes_géométriques",
    suite=["community"],
    prompt_function=custom_prompt.bbh_geometric_shapes,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="formes_géométriques",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_hyperbaton_community = LightevalTaskConfig(
    name="bbh_fr:hyperbate",
    suite=["community"],
    prompt_function=custom_prompt.bbh_hyperbaton,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="hyperbate",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_logical_deduction_five_objects_community = LightevalTaskConfig(
    name="bbh_fr:suivi_objets_mélangés_cinq_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_logical_deduction_five_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="suivi_objets_mélangés_cinq_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_logical_deduction_seven_objects_community = LightevalTaskConfig(
    name="bbh_fr:déduction_logique_sept_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_logical_deduction_seven_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="déduction_logique_sept_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_logical_deduction_three_objects_community = LightevalTaskConfig(
    name="bbh_fr:déduction_logique_trois_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_logical_deduction_three_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="déduction_logique_trois_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_movie_recommendation_community = LightevalTaskConfig(
    name="bbh_fr:recommandation_de_film",
    suite=["community"],
    prompt_function=custom_prompt.bbh_movie_recommendation,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="recommandation_de_film",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_multistep_arithmetic_two_community = LightevalTaskConfig(
    name="bbh_fr:multistep_arithmetic_two",
    suite=["community"],
    prompt_function=custom_prompt.bbh_multistep_arithmetic_two,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="multistep_arithmetic_two",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_navigate_community = LightevalTaskConfig(
    name="bbh_fr:naviguer",
    suite=["community"],
    prompt_function=custom_prompt.bbh_navigate,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="naviguer",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_object_counting_community = LightevalTaskConfig(
    name="bbh_fr:comptage_d_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_object_counting,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="comptage_d_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_penguins_in_a_table_community = LightevalTaskConfig(
    name="bbh_fr:pingouins_sur_une_table",
    suite=["community"],
    prompt_function=custom_prompt.bbh_penguins_in_a_table,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="pingouins_sur_une_table",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_reasoning_about_colored_objects_community = LightevalTaskConfig(
    name="bbh_fr:raisonnement_sur_les_objets_colorés",
    suite=["community"],
    prompt_function=custom_prompt.bbh_reasoning_about_colored_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="raisonnement_sur_les_objets_colorés",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_ruin_names_community = LightevalTaskConfig(
    name="bbh_fr:noms_de_ruines",
    suite=["community"],
    prompt_function=custom_prompt.bbh_ruin_names,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="noms_de_ruines",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_salient_translation_error_detection_community = LightevalTaskConfig(
    name="bbh_fr:détection_d_erreur_de_traduction_sailante",
    suite=["community"],
    prompt_function=custom_prompt.bbh_salient_translation_error_detection,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="détection_d_erreur_de_traduction_sailante",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_snarks_community = LightevalTaskConfig(
    name="bbh_fr:sarcasmes",
    suite=["community"],
    prompt_function=custom_prompt.bbh_snarks,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="sarcasmes",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_sports_understanding_community = LightevalTaskConfig(
    name="bbh_fr:compréhension_des_sports",
    suite=["community"],
    prompt_function=custom_prompt.bbh_sports_understanding,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="compréhension_des_sports",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_temporal_sequences_community = LightevalTaskConfig(
    name="bbh_fr:séquences_temporelles",
    suite=["community"],
    prompt_function=custom_prompt.bbh_temporal_sequences,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="séquences_temporelles",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_tracking_shuffled_objects_five_objects_community = LightevalTaskConfig(
    name="bbh_fr:suivi_objets_mélangés_cinq_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_tracking_shuffled_objects_five_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="suivi_objets_mélangés_cinq_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_tracking_shuffled_objects_seven_objects_community = LightevalTaskConfig(
    name="bbh_fr:suivi_objets_mélangés_sept_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_tracking_shuffled_objects_seven_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="suivi_objets_mélangés_sept_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_tracking_shuffled_objects_three_objects_community = LightevalTaskConfig(
    name="bbh_fr:suivi_objets_mélangés_trois_objets",
    suite=["community"],
    prompt_function=custom_prompt.bbh_tracking_shuffled_objects_three_objects,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="suivi_objets_mélangés_trois_objets",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_web_of_lies_community = LightevalTaskConfig(
    name="bbh_fr:toile_de_mensonges",
    suite=["community"],
    prompt_function=custom_prompt.bbh_web_of_lies,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="toile_de_mensonges",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)
bbh_word_sorting_community = LightevalTaskConfig(
    name="bbh_fr:tri_de_mots",
    suite=["community"],
    prompt_function=custom_prompt.bbh_word_sorting,
    hf_repo="le-leadboard/bbh_fr",
    hf_subset="tri_de_mots",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split="train",
    few_shots_select="random",
    generation_size=20 if not enable_thinking else 4096,
    metric=metrics,
    stop_sequence=["</s>", "Q=", "\n\n"] if not enable_thinking else [],
    trust_dataset=True,
    version=0,
)

# Adjusted English Tasks
math_500_fixed = LightevalTaskConfig(
    name="math_500",
    suite=["community"],
    prompt_function=default_prompts.math_500,
    hf_repo="HuggingFaceH4/MATH-500",
    hf_subset="default",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        Metrics.math_pass_at_1_1n,
    ],
    version=2,
)

gpqa_diamond_instruct_lighteval_fixed = LightevalTaskConfig(
    name="gpqa:diamond",
    suite=["community"],
    prompt_function=default_prompts.gpqa_instruct,
    hf_repo="Idavidrein/gpqa",
    hf_subset="gpqa_diamond",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        Metrics.gpqa_instruct_pass_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)

gpqa_extended_instruct_lighteval_fixed = LightevalTaskConfig(
    name="gpqa:extended",
    suite=["community"],
    prompt_function=default_prompts.gpqa_instruct,
    hf_repo="Idavidrein/gpqa",
    hf_subset="gpqa_extended",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        Metrics.gpqa_instruct_pass_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)
gpqa_main_instruct_lighteval_fixed = LightevalTaskConfig(
    name="gpqa:main",
    suite=["community"],
    prompt_function=default_prompts.gpqa_instruct,
    hf_repo="Idavidrein/gpqa",
    hf_subset="gpqa_main",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        Metrics.gpqa_instruct_pass_at_1_1n,
    ],
    stop_sequence=[],
    trust_dataset=True,
    version=1,
)

aime24_fixed = LightevalTaskConfig(
    name="aime24",
    suite=["community"],
    prompt_function=default_prompts.aime_prompt_fn,
    hf_repo="HuggingFaceH4/aime_2024",
    hf_subset="default",
    hf_avail_splits=["train"],
    evaluation_splits=["train"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=1024 if not enable_thinking else 8192,
    metric=[
        Metrics.math_pass_at_1_1n,
    ],
    version=2,
)

# STORE YOUR EVALS
TASKS_TABLE = [
    arc_challenge_fr_task,
    aime24_fr_task,
    math_500_fr_task,
    kholle_fr_task,
    math_lvl5_fr_task,
    ifeval_fr_task,
    gpqa_diamond_fr_task,
    gpqa_extended_fr_task,
    gpqa_main_fr_task,
    boolq_fr_task,
    mmlu_fr_task,
    musr_fr_team_allocation,
    musr_fr_object_placements,
    musr_fr_murder_mysteries,
    bbh_boolean_expressions_community,
    bbh_causal_judgment_community,
    bbh_date_understanding_community,
    bbh_disambiguation_qa_community,
    bbh_dyck_languages_community,
    bbh_formal_fallacies_community,
    bbh_geometric_shapes_community,
    bbh_hyperbaton_community,
    bbh_logical_deduction_five_objects_community,
    bbh_logical_deduction_seven_objects_community,
    bbh_logical_deduction_three_objects_community,
    bbh_movie_recommendation_community,
    bbh_multistep_arithmetic_two_community,
    bbh_navigate_community,
    bbh_object_counting_community,
    bbh_penguins_in_a_table_community,
    bbh_reasoning_about_colored_objects_community,
    bbh_ruin_names_community,
    bbh_salient_translation_error_detection_community,
    bbh_snarks_community,
    bbh_sports_understanding_community,
    bbh_temporal_sequences_community,
    bbh_tracking_shuffled_objects_five_objects_community,
    bbh_tracking_shuffled_objects_seven_objects_community,
    bbh_tracking_shuffled_objects_three_objects_community,
    bbh_web_of_lies_community,
    bbh_word_sorting_community,
    hellaswag_fr_task,
    math_500_fixed,
    gpqa_diamond_instruct_lighteval_fixed,
    gpqa_extended_instruct_lighteval_fixed,
    gpqa_main_instruct_lighteval_fixed,
    aime24_fixed,
]
